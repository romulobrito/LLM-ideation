# Normaliza√ß√£o de Embeddings e Escolha de Dist√¢ncia

## Pergunta do Igor

> "Pensando aqui... ser√° que n√£o compensa usar a dist√¢ncia euclidiana em vez da dist√¢ncia do cosseno no espa√ßo de embeddings? A do cosseno s√≥ olha √¢ngulos n√©? Pode ser que a magnitude do vetor tenha import√¢ncia tamb√©m."

> "Entendo... s√≥ que a pergunta passa a ser ent√£o: ser√° que √© bom normalizar os vetores?"

## Resposta

### ‚úÖ Sim, √© padr√£o normalizar embeddings para similaridade sem√¢ntica

**Raz√£o principal**: Eliminar o vi√©s de comprimento do texto, focando apenas na sem√¢ntica pura.

---

## üìö Fundamenta√ß√£o Te√≥rica

### 1. Sentence-Transformers normaliza por padr√£o

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# Normaliza√ß√£o expl√≠cita (recomendado)
embeddings = model.encode(texts, normalize_embeddings=True)

# Verificar (deve dar ~1.0)
import numpy as np
print(np.linalg.norm(embeddings[0]))  # Output: 1.0000
```

**Refer√™ncia**: Reimers & Gurevych (2019) - "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
- Paper original usa cosine similarity como m√©trica padr√£o
- Documenta√ß√£o oficial: https://www.sbert.net/docs/usage/semantic_textual_similarity.html

---

## üî¨ Por que normalizar?

### Problema: Vi√©s de comprimento

**SEM normaliza√ß√£o**:
- Textos longos tendem a ter magnitude maior
- Textos curtos tendem a ter magnitude menor
- Dist√¢ncia euclidiana √© afetada por esse vi√©s

**Exemplo pr√°tico**:
```python
# Textos semanticamente id√™nticos, mas tamanhos diferentes
texto_curto = "O gato dormiu."  # 3 palavras
texto_longo = "O gato dormiu profundamente no sof√° confort√°vel da sala durante toda a tarde ensolarada de domingo."  # 16 palavras

# SEM normaliza√ß√£o
embeddings = model.encode([texto_curto, texto_longo], normalize_embeddings=False)
print(f"Norma (curto): {np.linalg.norm(embeddings[0]):.4f}")  # ~25.3
print(f"Norma (longo): {np.linalg.norm(embeddings[1]):.4f}")  # ~27.8

# COM normaliza√ß√£o
embeddings_norm = model.encode([texto_curto, texto_longo], normalize_embeddings=True)
print(f"Norma (curto): {np.linalg.norm(embeddings_norm[0]):.4f}")  # 1.0000
print(f"Norma (longo): {np.linalg.norm(embeddings_norm[1]):.4f}")  # 1.0000
```

**Observa√ß√£o importante**: 
- Sentence-Transformers usa **mean pooling**, ent√£o o vi√©s √© MENOR que em TF-IDF ou Word2Vec
- Mas ainda existe um vi√©s residual, especialmente para textos muito curtos/longos
- **Normalizar elimina completamente esse efeito**

---

## üìê Equival√™ncia matem√°tica (vetores normalizados)

Para vetores unit√°rios (||v|| = 1):

```
dist√¢ncia_euclidiana(A, B)¬≤ = ||A - B||¬≤
                             = ||A||¬≤ + ||B||¬≤ - 2‚ü®A, B‚ü©
                             = 1 + 1 - 2‚ü®A, B‚ü©
                             = 2(1 - ‚ü®A, B‚ü©)
                             = 2(1 - cosine_similarity(A, B))
                             = 2 * cosine_distance(A, B)
```

**Conclus√£o**: Para vetores normalizados, Euclidiana e Cosseno s√£o **monotonicamente equivalentes**.
- Se `cos_dist(A, B) < cos_dist(C, D)`, ent√£o `eucl_dist(A, B) < eucl_dist(C, D)`
- A ordem dos vizinhos mais pr√≥ximos √© ID√äNTICA

---

## üéØ No nosso experimento espec√≠fico

### Configura√ß√£o atual:
- **Prompt controla comprimento**: ~150 palavras por texto
- **Varia√ß√£o esperada**: 145-155 palavras (pequena)
- **Vi√©s de comprimento**: M√çNIMO mesmo sem normaliza√ß√£o

### Mas normalizamos porque:

1. **Boa pr√°tica**: Garante comparabilidade com literatura
2. **Robustez**: Se LLM gerar textos fora do padr√£o (50 ou 200 palavras), n√£o afeta
3. **Consist√™ncia**: `experiment_iterativo.py` j√° normaliza, ent√£o `visualizar_experimento.py` deve fazer o mesmo
4. **Interpreta√ß√£o**: Dist√¢ncias refletem **sem√¢ntica pura**, n√£o magnitude

---

## üîç Verifica√ß√£o no c√≥digo

### Arquivos que usam embeddings:

| Arquivo | Linha | Normaliza√ß√£o? |
|---------|-------|---------------|
| `experiment_iterativo.py` | 108 | ‚úÖ `normalize_embeddings=True` |
| `app_streamlit_metrics.py` | 557, 558, 563, 564 | ‚úÖ `normalize_embeddings=True` |
| `visualizar_experimento.py` | 385, 535, 2021, 2027 | ‚úÖ `normalize_embeddings=True` (corrigido) |

**Status**: ‚úÖ Todos os arquivos agora normalizam consistentemente

---

## üìñ Refer√™ncias

1. **Reimers, N., & Gurevych, I. (2019)**. "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks". arXiv:1908.10084.
   - Usa cosine similarity como m√©trica padr√£o
   - Normaliza√ß√£o impl√≠cita na compara√ß√£o

2. **Documenta√ß√£o Sentence-Transformers**:
   - https://www.sbert.net/docs/usage/semantic_textual_similarity.html
   - Recomenda `util.cos_sim()` para compara√ß√£o sem√¢ntica

3. **Mikolov et al. (2013)** - "Efficient Estimation of Word Representations in Vector Space"
   - Word2Vec: magnitude pode ter significado (n√£o normalizar)
   - Diferente de BERT: magnitude n√£o √© informativa (normalizar)

---

## üéì Conclus√£o

**Pergunta do Igor**: "Ser√° que n√£o compensa usar dist√¢ncia euclidiana?"
- **Resposta**: Para vetores normalizados, Euclidiana e Cosseno s√£o equivalentes.

**Pergunta do Igor**: "Ser√° que √© bom normalizar os vetores?"
- **Resposta**: SIM. Elimina vi√©s de comprimento e garante que medimos sem√¢ntica pura.

**Resposta do Romulo**: "Acredito ser usual nestas tarefas."
- **Status**: ‚úÖ **CORRETO**! Normaliza√ß√£o √© padr√£o em tarefas de similaridade sem√¢ntica com Sentence-Transformers.

---

## üí° Quando N√ÉO normalizar seria √∫til

- **Word2Vec/GloVe raw**: Magnitude pode indicar "import√¢ncia" da palavra
- **Embeddings de imagens**: Magnitude pode refletir "confian√ßa" da rede
- **Triplet loss espec√≠fico**: Se o modelo foi treinado para usar magnitude
- **Nosso caso**: ‚ùå N√£o se aplica - queremos sem√¢ntica pura

