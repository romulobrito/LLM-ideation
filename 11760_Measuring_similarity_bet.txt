Under review as a conference paper at ICLR 2025

MEASURING SIMILARITY BETWEEN EMBEDDING
SPACES USING INDUCED NEIGHBORHOOD GRAPHS

Anonymous authors
Paper under double-blind review

ABSTRACT

Deep Learning techniques have excelled at generating embedding spaces that cap-
ture semantic similarities between items. Often these representations are paired,
enabling experiments with analogies (pairs within the same domain) and cross-
modality (pairs across domains). These experiments are based on specific assump-
tions about the geometry of embedding spaces, which allow finding paired items by
extrapolating the positional relationships between embedding pairs in the training
dataset, allowing for tasks such as finding new analogies, and multimodal zero-shot
classification. In this work, we propose a metric to evaluate the similarity between
paired item representations. Our proposal is built from the structural similarity
between the nearest-neighbors induced graphs of each representation, and can be
configured to compare spaces based on different distance metrics and on different
neighborhood sizes. We demonstrate that our proposal can be used to identify simi-
lar structures at different scales, which is hard to achieve with kernel methods such
as Centered Kernel Alignment (CKA). We further illustrate our method with two
case studies: an analogy task using GloVe embeddings, and zero-shot classification
in using CLIP and BLIP-2 embeddings. Our results show that accuracy in both
analogy and zero-shot classification tasks correlates with the embedding similarity.
These findings can help explain performance differences in these tasks, and may
lead to improved design of paired-embedding models in the future.

1

INTRODUCTION

Several tasks in machine learning rely on obtaining pairs of corresponding representations from
the same item in the same or different domains, such as word pairs in an analogy task (where the
underlying item is the analogy itself) or items from different media or text modalities, such as text-to-
image cross-modal representations. Each item representation is usually converted to an embedding
vector in a space associated with its respective domain. Higher performances in these paired-item
tasks are often linked to the preservation of proximity between embeddings related to the same item
as we cross from one space of embeddings to the other. Thus, the study of how proximity is preserved
between spaces may provide deeper insights into the reasons underlying performance differences in
these tasks.

Contributions1 In this work, we introduce a method, namely Nearest Neighbor Graph Similarity
(NNGS), to measure the similarity between paired embeddings of items based on the estimated
Jaccard similarity between the neighborhoods of induced graphs (Donnat and Holmes, 2018). We
show that the neighborhood size can be adjusted so that it reflects the locality of changes observed in
the similarity measure. In two case studies, we show that the proposed metric can provide insight
on the reasons underlying performance in both the analogy and the zero-shot classification tasks.
In special, it can be used to identify categories of words that are not represented adequately for
the analogy task, and to identify prompt templates that can lead to higher accuracy in zero-shot
classification.

1Code for the method and for the experiments in this paper can be found at:
https://anonymous.4open.science/r/graph_structure-F301 and
https://anonymous.4open.science/r/graph_structure-F301/experiments/nngs_
paper/README.md.

1

000
001

002
003
004
005
006
007

008
009
010
011
012
013

014
015
016
017
018

019
020
021
022
023
024

025
026
027
028
029
030

031
032
033
034
035
036

037
038
039
040
041

042
043
044
045
046
047

048
049
050
051
052
053

Under review as a conference paper at ICLR 2025

2 RELATED WORK

2.1 EMBEDDINGS

Semantic Word Embeddings Prior research involving embedding spaces has found usefulness in
particular topological characteristics, such as the one spanned by GloVe (Pennington et al., 2014). In
GloVe, each word from a vocabulary is represented by a vector eword in a so-called semantic space.
Within a semantic space, a change of meaning is represented by a translation vector, that is, stating
that the relationship between “a” and “b” is the same as that between “c” and “d” means that their
embeddings have the relationship:

eb − ea = ed − ec = ecategory A→category B.

(1)

In Equation 1, ecategory A→category B is a vector that maps elements from one category to another (e.g.,
countries to capitals, adjectives to corresponding superlatives, etc.) (Ethayarajh et al., 2018).

Cross-modal Embeddings More recently, multimodal embedding spaces have been used to represent
items according to their content in different modalities. In CLIP (Radford et al., 2021), for example,
the image corresponding to a photo of a car is represented by a vector ei, the text “a photo of a
car” is represented by et and, ideally, ei ≈ et. In BLIP-2 (Li et al., 2023), the multimodal models
are obtained by training unimodal language models that map onto each other using an attention
mechanism. These ideas allow zero-shot classification by calculating the embeddings of an unknown
image and comparing it to the embeddings of several candidate texts, and then choosing the most
similar text as the class for that image.

Assessing embeddings Traditionally, the properties of embedding spaces have been evaluated
using accuracy in task-specific benchmarks (Faruqui et al., 2016). This is important, as it allows a
straightforward comparison between systems and a direct measure of its utility, but leaves gaps that
were only addressed later, like the requirement for exponential data (Udandarao et al., 2024) or the
tendency to behave like a simple bag-of-words (Yuksekgonul et al., 2023) in cross-modal zero-shot
learning and the propensity to biasing embedding spaces according to term frequencies (Faruqui et al.,
2016). Importantly, in subsequent work, these embeddings have been analyzed to find properties that
could help explain their performances, such as studies on bounds for linear analogies (Ethayarajh
et al., 2018) in GloVe and the properties of partial orthogonality (Jiang et al., 2023) and linear
compositionality in CLIP (Trager et al., 2023).

2.2 SIMILARITY BETWEEN REPRESENTATIONS

Items from a dataset can have different representations. The similarity between representations can
be measured with a similarity index s(X, Y ), which compares different representations X ∈ Rn×p1
and Y ∈ Rn×p2 (with possibly different dimensionalities p1 and p2, with p1 ≤ p2 without loss of
generality) of the same set of n items (Kornblith et al., 2019).

Arguably, a similarity index should be invariant to orthonormal linear transformations and isotropic
non-zero scaling and translation (Kornblith et al., 2019). More generally, a similarity index could
be invariant to general invertible linear transformations (Raghu et al., 2017), but this property will
not be adopted in the present work as it implies that all datasets have the same similarity index if
p1 > n (Kornblith et al., 2019).

An important measure for similarity between representation is the Centered Kernel Alignment (CKA)
measure (Kornblith et al., 2019), calculated by:

CKA(X, Y ) =

(cid:113)

tr(K c

y(Y c))

x(X c)K c
x(X c))tr(K c

tr(K c

x(X c)K c

y(Y c)K c

y(Y c))

,

(2)

where tr(.) is the trace operator, Kx and Ky are kernel functions applied to X and Y , and the
c operation consists of centering a cloud point by subtracting the dimension-wise mean from all
elements.

CKA draws from the idea of assessing how distances between items change from one representation
to another. With a linear kernel, a changes in the distance between data blobs have a larger impact

2

054
055

056
057
058
059
060
061

062
063
064
065
066
067

068
069
070
071
072

073
074
075
076
077
078

079
080
081
082
083
084

085
086
087
088
089
090

091
092
093
094
095

096
097
098
099
100
101

102
103
104
105
106
107

Under review as a conference paper at ICLR 2025

than the distance between points within each blob, because distance changes are usually larger. By
using an RBF kernel (exp −D(X)/2σ2, where D(X) is a matrix with pairwise distances between
the items of X) it is possible to give more weight to changes made on smaller distances.

However, the weighting process is distance-based, that is, the choice of an adequate value for σ
depends on the radius of the localities being analyzed. This value is usually hard to consistently
obtain, as it can change even if data is subject to isotropic transformations. Also, it can have little
meaning if data is spread in irregular manifolds.

These aspects do not necessarily constitute limitations of CKA. Rather, they are a consequence of the
definition of similarity used to build the metric. A different approach, called GULP (Boix-Adsera
et al., 2022), defines similar representations as those that lead to the same predictive performance for
a set of items, which has shown increased results, when compared to CKA, in the task of predicting
neural network layers using the CIFAR-10 dataset (Krizhevsky and Hinton, 2009). It is define in
terms of the centralized representations X c and Y c as:

i=1(X c)T X c
i=1(Y c)T Y c
i=1(X c)T Y c

(cid:80)N
ˆΣX = 1
n
(cid:80)N
ˆΣY = 1
n
(cid:80)N
ˆΣXY = 1
n
X = ( ˆΣX + λI)−1
ˆΣ−λ
= ( ˆΣY + λI)−1
ˆΣ−λ
Y
ˆΣX ˆΣ−λ
GULP(X, Y, λ) = tr( ˆΣ−λ
X
X

ˆΣX ) + tr( ˆΣ−λ
Y

ˆΣY ˆΣ−λ
Y

ˆΣY ) − 2tr( ˆΣ−λ
X

ˆΣXY ˆΣ−λ
Y

ˆΣT

XY ),

(3)

where λ is a regularization parameter.

Another definition of similarity, namely ContraSIM (Rahamim and Belinkov, 2023), relies on the
idea that it should be possible to map similar representations to a common embedding space using
contrastive loss. Thus, ContraSIM (Rahamim and Belinkov, 2023) uses the loss of such mapping as a
similarity measure. ContraSIM was evaluated using images from the CIFAR-10 (Krizhevsky and
Hinton, 2009) and CIFAR-100 (Krizhevsky et al.) datasets.

We observe that both GULP and ContraSIM are extensions of the Canonical Correlation Analysis
(CCA) (Hotelling, 1992), as they measure the error obtained due to projecting the representations X
and Y onto a common space. ContraSIM (Rahamim and Belinkov, 2023) allows greater flexibility in
this projection by allowing the use of arbitrary encoders, but this also implies in the need to train an
encoder. Training contrastive losses implies in a series of particular problems and, because of that,
ContraSIM was not used in further experiments.

2.3 STRUCTURAL SIMILARITY BETWEEN GRAPH NODES

Given a point cloud V = {vi}, vi ∈ Rd, a directed graph Gk = (V, Ek) can be induced by connecting
each point to its k closest neighbors (Eppstein et al., 1997), k ∈ [1, n − 1], thus forming the set of
directed edges Ek. A point is not allowed to have itself as a neighbor, thus Gk is a directed simple
graph. We say that Gk is induced by k-neighborhood in V . The set of neighbor indices of a vertex
vi is Nk(vi) = {j|(i, j) ∈ Ek}. Two graph vertices vi and vj, i ̸= j, can be said to be structurally
equivalent if they share the same set of neighbors, that is, N (vi) = N (vj) (Newman, 2018). If
they only share some neighbors, we can use the Jaccard similarity J(N (vi), N (vj)) (Equation 4) to
measure the degree of structural similarity between vi and vj (Donnat and Holmes, 2018):

J(Nk(vi), Nk(vj)) =

|Nk(vi) ∩ Nk(vj)|
|Nk(vi) ∪ Nk(vj)|

.

(4)

A soft approach to graph structural similarity has shown to lead to minor accuracy increases in
downstream classifiers based on image-text pre-trained embeddings (Sobal et al., 2024). In this work,
we further assess how this idea can relate to zero-shot learning tasks in this same scenario. Also, we
bring evidence that this type of approach can relate to task performance in other tasks such as an
embedding-space analogy task.

3

108
109

110
111
112
113
114
115

116
117
118
119
120
121

122
123
124
125
126

127
128
129
130
131
132

133
134
135
136
137
138

139
140
141
142
143
144

145
146
147
148
149

150
151
152
153
154
155

156
157
158
159
160
161

Under review as a conference paper at ICLR 2025

162
163

164
165
166
167
168
169

170
171
172
173
174
175

176
177
178
179
180

181
182
183
184
185
186

187
188
189
190
191
192

193
194
195
196
197
198

199
200
201
202
203

204
205
206
207
208
209

210
211
212
213
214
215

3 PROPOSED METHOD: NEAREST NEIGHBORHOOD GRAPH SIMILARITY

In this section, we propose Nearest Neighborhood Graph Similarity (NNGS). We discuss some some
theoretical perspectives that explain how to interpret its results.

3.1 SIMILARITY BETWEEN CORRESPONDING POINT CLOUDS

The notion of structural similarity between nodes (Equation 4) can be adapted to compare point
clouds where there is a one-to-one correspondence between points in both clouds2. For such, let
X = {xi} and Y = {yi}, i ∈ {1, 2, · · · , n}, be point clouds where xi ∈ Rp1 and yi ∈ Rp2 are
corresponding points. Two graphs, GX,k = (X, EX,k) and GY,k = (Y, EY,k), can be induced by
k-neighborhood on X and Y as described previously.

Let NX,k(xi) be the set of indexes of the k points closest to xi in X, and define NY,k(yi) analogously.
Importantly, the distance measure used for this operation can be selected according to the problem.
The structural similarity between corresponding points xi and yi is defined as:

J(NX,k(xi), NY,k(yi)) =

|NX,k(xi) ∩ NY,k(yi)|
|NX,k(xi) ∪ NY,k(yi)|

.

(5)

Note that, different from Equation 4, points xi and yi do not belong to the same graph, hence the
need for comparing their sets of neighbor indices, and not the neighboring points themselves.

From Equation 5, we define the Nearest Neighborhood Graph Similarity (NNGS) between point
clouds X and Y as the average structural similarity between corresponding points:

NNGS(X, Y, k) =

1
n

n
(cid:88)

i=1

J(NX,k(xi), NY,k(yi)).

(6)

Note that the structural similarity between point clouds depends only on their neighborhood structure:
the precise definitions of distance in each domain, or the value of point coordinates only impact the
structural similarity if they change the ranks of closest points among the cloud. For instance, if the
point clouds are equal, that is, X = Y , then NNGS(X, Y, k) = 1, ∀k ∈ [1, n − 1]. Furthermore,
NNGS(X, Y, k) = NNGS(X, Y ′, k) if Y ′ is constructed from Y by applying only isotropic scaling,
translations, or orthonormal transformations, as they not change the ranks of neighborhood distances
between points.

3.2 PROPERTIES OF NNGS

In addition to the invariance to isotropic scaling, translations, and orthonormal transformations,
NNGS has important properties related to its behavior regarding white noise, and invariance properties
related to the number n of points in the evaluated set and their dimensionality (p1 and p2). These
properties are discussed next. In the following experiments, we used the Cosine similarity to define
neighborhoods.

3.2.1 SIMILARITY OF RANDOM POINT CLOUDS

If X = {xi} iid∼ N (0, IdX ), Y = {yi} iid∼ N (0, IdY ), then the neighborhoods NX,k(xi) and NY,k(yi)
become random draws. In this case, the intersection cardinality |NX,k(xi) ∩ NY,k(yi)| for a randomly
chosen i follows a hypergeometric distribution with n − 1 total elements, k elements of interest
and k draws. Therefore, J(NX,k(xi), NY,k(yi)) is a random variable whose expected value over all
possible pairs of point clouds is (see proof in Appendix B):

E[J(NX,k(xi), NY,k(yi)] ≥ H(k) =

k
2(n − 1) − k

.

(7)

Importantly, E[NNGS(X, Y, k)] ≥ E[J(NX,k(xi), NY,k(yi)] in this case because the neighborhoods
between points used to calculate J in Equation 5 are not independent. Rather, the choice of the first

2This situation can be found in multimodal datasets used to train, e.g., CLIP.

4

Under review as a conference paper at ICLR 2025

neighborhoods impact the remaining ones by restricting the number of possible choices that maintain
the symmetry property of distances (see Appendix C for a counter-example of independence). Hence,
H(k) can be seen as a lower bound for E[NNGS(X, Y, k)] if point clouds X and Y are i.i.d.

In most practical cases, however, the point clouds X and Y are neither equal nor entirely independent.
These cases are further discussed next.

3.2.2 SIMILARITY IN THE PRESENCE OF NOISE

One way to progressively distort the structure of a point cloud is by adding white noise. For such, we
generate a point cloud X = {xi} iid∼ N (0, I). After that, we define a desired Signal-to-Noise Ratio
(SNR) and calculate Y = {yi} = {xi + αϕi}, where {ϕi} iid∼ N (0, I) and α = 10− SNR
20 .
With low noise, SNR → ∞, α → 0 and X → Y , thus NNGS(X, Y, k) → 1. However, in a very high
noise scenario, SNR → −∞, causing X and Y to become uncorrelated, thus NNGS(X, Y, k) →
H(k). As shown in Figure 1, intermediate SNR values lead to curves that are between the other two,
but are clearly not simple linear interpolations of the extreme values.

Figure 1: Mean structural similarity in the presence of white noise. The point clouds were generated
using n = 100 points and d = 50 dimensions. The impact of changing n and d are respectively
discussed in Section 3.2.3 and Section 3.2.4. The curves were bootstrapped as to obtain mean and
standard deviation, and the depicted intervals correspond to two standard deviations. With very
low SNRs, NNGS(X, Y, k) → H(k), while in high SNRs NNGS(X, Y, k) → 1. An SNR sweep is
provided in Appendix D.

lower values of k refer to a close vicinity, whereas larger values of k make
Importantly,
NNGS(X, Y, k) refer to a more general vicinity in the point clouds. In Figure 1, the similarity
curves do not cross, indicating that the similarity is equally distorted in all vicinity sizes. Although
this suggests that any value of k could be equally useful to evaluate NNGS(X, Y, k), the non-crossing
behavior is a property specific to distortions caused by additive random noise.

3.2.3 CHANGING THE POINT CLOUD SIZE n

In Equation 7, if c = k/(n − 1), then:

H(k) =

k
2(n − 1) − k

=

k
n−1
2 − k

(n−1)

=

c
2 − c

,

(8)

thus, H(k) only depends on the relative neighborhood size c = k/(n − 1).

Figure 2 shows that NNGS(X, Y, k) does not change with n if c is kept constant, that is, k = ⌊c(n −
1)⌋ for each n. This observed behavior allows comparing NNGS(X, Y, k) between experiments with
different point cloud size n as long as the corresponding values of c = k/(n − 1) are close in value.

In this case, using small values for c makes NNGS(X, Y, k) focus on a local neighborhood, whereas
values of c closer to 1 shift NNGS(X, Y, k) towards assessing a more global structure.

5

216
217

218
219
220
221
222
223

224
225
226
227
228
229

230
231
232
233
234

235
236
237
238
239
240

241
242
243
244
245
246

247
248
249
250
251
252

253
254
255
256
257

258
259
260
261
262
263

264
265
266
267
268
269

020406080100k0.00.20.40.60.81.0NNGS(X,Y,k)-20 dB-10 dB0 dB10 dB20 dB30 dB40 dBH(k)Under review as a conference paper at ICLR 2025

270
271

272
273
274
275
276
277

278
279
280
281
282
283

284
285
286
287
288

289
290
291
292
293
294

295
296
297
298
299
300

301
302
303
304
305
306

307
308
309
310
311

312
313
314
315
316
317

318
319
320
321
322
323

Figure 2: The similarity NNGS(X, Y, k) remains nearly constant as the point cloud size n increases
as long as k = ⌊c(n − 1)⌋ for a constant value of c. In this figure, we arbitrarily chose c = 0.2.

3.2.4

INVARIANCE TO POINT CLOUD DIMENSIONALITY (p1 AND p2)

We also investigate how NNGS(X, Y, k) is affected when the point cloud dimensionality (p1 and/or
p2) is changed. When the dimensionality increases, it is more likely that the cosine similarity
between random points becomes equal to zero, that is, all points are orthogonal to each other.
Thus, adding noise with a particular variance has an equal chance of taking a point to a different
neighborhood. Consequently, as shown in Figure 3, the dimensionality of the point clouds do not
impact NNGS(X, Y, k).

Figure 3: The similarity NNGS(X, Y, k) is not affected by the increase of dimensionality, except
in very low dimensionality settings (less than 10). In this figure, we fixed k = 20 and the point
cloud size n = 100. This is a consequence of the fact that random points in higher dimensionalities
are likely to be orthogonal to each other, thus the noise variance required to get closer to different
neighbors is similar regardless of the increased dimensionality.

This further indicates a consistency of this measure when evaluating point clouds in different
dimensionalities - for example, assessing the distortions caused by non-linear projections in multilayer
neural networks.

3.3 COMPARING NNGS, CKA, AND GULP

NNGS has two additional parameters when compared to CKA: the neighborhood size k, and the
distance metric used to induce the neighborhood graph. GULP has one additional parameter: the
regularization term λ. In datasets with more than one cluster, these parameters can be manipulated to
find similarities at different scales and in different situations. The experiments below were conducted
with toy datasets crafted especially to demonstrate these differences, as follows.

6

255075100125150175200n0.00.20.40.60.81.0NNGS(X,Y,0.2(n1))-20 dB-10 dB0 dB10 dB20 dB30 dB40 dB0102030405060708090100dim0.00.20.40.60.81.0NNGS(X,Y,20)-20 dB-10 dB0 dB10 dB20 dB30 dB40 dBUnder review as a conference paper at ICLR 2025

Data representations with blobs The latent representation of data points is frequently consisted of
blobs, especially in the context of supervised classifiers. In this context, each blob corresponds to a
different class. As will be discussed next, NNGS, CKA, and GULP behave differently depending on
the nature of transformations applied to blobs. The algorithm and parameters used to create the data
blobs are shown in Appendix E. The results for each experiment are shown in Table 1 and discussed
next.

Method
Kernel / distance
Parameter

CKA
Linear
-

CKA RBF
RBF
σ = 0.01

CKA RBF

NNGS
NNGS
RBF Minkowski Minkowski
k = 300
k = 5

σ = 3

GULP
-
λ = 0.01

GULP
-
λ = 1

Blobs with Different Scales
Unbal. Blobs w/ Dif. Scales
Noise Within Blobs
Shuffled Blobs

0.86
0.10
0.99
0.16

1.00
1.00
1.00
1.00

0.98
0.31
0.96
0.84

1.00
1.00
0.03
1.00

0.76
0.99
0.99
0.63

15.43
1.50
6.88
2.07

0.11
0.02
0.02
0.49

Table 1: Similarity between artificial aligned datasets measured by variants of CKA, NNGS, and
GULP. By changing k in NNGS, it is possible to change the locality in which the similarity is
measured. However, this is harder to obtain by changing the σ value in CKA with an RBF kernel or
the λ parameter in GULP even if they follow the exact parameters used in the dataset creation.

Blobs with different scales A particularly interesting case is that in which representation X
is consisted of two data blobs and they are scaled differently to obtain Y . We observe that
RBF CKA(σ = 0.1) and NNGS(k = 5) find a similarity of 1, as blobs are locally unchanged.
NNGS(k = 300) finds a small modification in the inter-cluster neighborhoods, whereas Linear
CKA is slightly affected by the asymetric scaling. GULP assumes values between 0.11 and 15.43
depending on the regluarization factor λ, which makes it hard to interpret what is a high or a low
value in the context of this measure.

Unbalanced blobs with different scales The same asymetric scaling used above can be applied when
blobs have an unbalanced number of items. In this case, The average distance modification within the
dataset is not preserved, whereas the closest neighborhoods are preserved. Compared to the balanced
experiment, NNGS for a local neighborhood remains unchanged, whereas both Linear and RBF CKA
for a more global neighborhood are affected. We observe that NNGS for a broader neighborhood is
changed due to the modifications in the blob size. GULP presents low values, indicating a behavior
similar to Linear CKA.

Noise within blobs Conversely, if an average amount of noise is added to X so that Y preserves
the blob centroids, but shuffles the local neighborhood, CKA returns a high similarity between both
representations, thus failing to identify the local noise due to the preservation of the higher-level
similarity. GULP with a high λ seems to be inaffected, whereas a low λ lead to an increased value of
similarity. Conversely, NNGS with a low k identifies that local neighborhoods were modified. Yet,
NNGS with a high k is able to identify that blob neighborhoods were changed.

Shuffling blob centroids If data is consisted of blobs, it is possible to transform the representation
X by applying a translation to each blob centroid, but without changing relative positions within
centroids, generating Y . In this case, long-range distances are highly impacted, whereas local
neighborhoods are preserved. Consequently, CKA and GULP find a low similarity value, while
NNGS with a low value for k identifies that blobs are locally unchanged.

These experiments show that changing the value of k implies in a modification of the locality of the
similarity measure. Although theoretically the value of σ in CKA with an RBF kernel plays the same
role, it is easier to find suitable values for k than to σ. In special, we observe that even very small
values of σ were innefective to find local changes in the point clouds.

In addition, we find that the values yielded by GULP are hard to classify as “high” or “low” as they
can be unbounded and highly dependent on the regularization parameter λ.

The adequate value of k can be obtained by standard techniques. These can be either observing
datasets properties, like the number of expected points per data blob, or observing data plots that
highlight data clusters. The value of k implies in the type of similarity (local or global) being
measured.

7

324
325

326
327
328
329
330
331

332
333
334
335
336
337

338
339
340
341
342

343
344
345
346
347
348

349
350
351
352
353
354

355
356
357
358
359
360

361
362
363
364
365

366
367
368
369
370
371

372
373
374
375
376
377

Under review as a conference paper at ICLR 2025

4 CASE STUDIES

The experiments described in this section3 aim to demonstrate how the similarity measure
NNGS(X, Y, k) (Equation 6) can be used to unveil properties of embedding spaces. The first
experiment use GloVe embeddings (Section 4.1) to find an association between the accuracy in anal-
ogy tasks and their corresponding similarity NNGS(X, Y, k). The second experiment (Section 4.2)
uses CLIP to generate text embeddings for candidate captions in a zero-shot classification task, and
highlights that the accuracy in this task are associated with the structural similarity NNGS(X, Y, k)
between the caption embeddings and the image embeddings.

4.1 SIMILARITY IN SEMANTIC WORD EMBEDDINGS

Equation 1 suggests that analogies between groups of items can be represented by a translation. Thus,
point clouds that are adequate for an analogy task based on Equation 1 are likely to present a high
structural similarity as calculated by Equation 6.

We evaluate this assumption using the 300-dimensions GloVE embeddings and the analogy tasks
defined in (Mikolov et al., 2013)4. For each analogy task, we find the two related point clouds
(for example: one containing embeddings for countries and the other containg embeddings for
capitals), and calculate the similarity between them using Equation 6 for various values of the relative
neighborhood size c. The results are shown in Figure 4.

Figure 4: Mean structural similarity in GloVe embeddings for each analogy task. The curves present
a shape similar to those in Figure 1, and some tasks clearly have greater similarity than others.

The curves present a behavior similar to those in Figure 1, but are more jittery, causing close curves
to cross at various points. This is because the associated point clouds are related by transforms that
allow Equation 1 to work, but also by non-uniform distortions due to the process of building the
embeddings. Next, we evaluate the association between the structural similarity and the accuracy of
each analogy task.

As shown in Figure 5, tasks in which NNGS(X, Y, k) are higher tend to have higher accuracy. We
observe a trend, as supported by the Pearson correlation between NNGS and the analogy accuracy
(Pearson’s ρ = 0.86, p < 10−4). Also, we observe that this trend seems to diminish for high analogy
accuracy values, which could be caused by the saturation of the accuracy measure.

Importantly, the same experiment using CKA presents, as well, a high correlation, whereas GULP
is unable to capture this trend. This is should not be confused with a flaw of GULP; rather, it is
an indication that the aspects that lead to a higher accuracy in the analogy task are unrelated to the

3The experiments were executed in an on-premises server. The experiments for the analogies task used a
single-core CPU with 256 GB RAM: it did not require a GPU and most of the execution time was spent loading
embeddings into memory. The CLIP model, used in the zero-shot-classification task, executed in a server with
256GB RAM, using a single NVIDIA Quadro RTX 6000 GPU with 24GB RAM. Experiments took around 1h
in total.

4The repository that contains this data (https://github.com/nicholas-leonard/word2vec/)

is licensed under the Apache licence

8

378
379

380
381
382
383
384
385

386
387
388
389
390
391

392
393
394
395
396

397
398
399
400
401
402

403
404
405
406
407
408

409
410
411
412
413
414

415
416
417
418
419

420
421
422
423
424
425

426
427
428
429
430
431

0.00.20.40.60.81.0c0.00.20.40.60.81.0NNGS(X,Y,cn)capital-common-countries, n=22capital-world, n=115currency, n=27city-in-state, n=26family, n=22gram1-adjective-to-adverb, n=31gram2-opposite, n=28gram3-comparative, n=36gram4-superlative, n=33gram5-present-participle, n=32gram6-nationality-adjective, n=40gram7-past-tense, n=39gram8-plural, n=36gram9-plural-verbs, n=29H(k)Under review as a conference paper at ICLR 2025

432
433

434
435
436
437
438
439

440
441
442
443
444
445

446
447
448
449
450

451
452
453
454
455
456

457
458
459
460
461
462

463
464
465
466
467
468

469
470
471
472
473

474
475
476
477
478
479

480
481
482
483
484
485

underlying ideas of GULP (similarity in the prediction accuracy). Hence, all three measures are
useful to draw a complete diagnosis of this behavior.

4.2 SIMILARITY IN CROSS-MODAL ZERO-SHOT LEARNING

In a cross-modal zero-shot classification task, the text embeddings are calculated from a set of
captions that are constructed from the class labels and template phrases. In CLIP, these templates
are variations of the phrase “a [description] [photo/image] of X”5, where “X” is substituted by the
desired label.

We perform experiments to draw insight about the relationship between the phrasing of text captions,
the zero-shot classification performance, and the similarity of cross-modal embeddings using the
pre-trained CLIP model available on HuggingFace6. In these experiments, we assess the similarity
between embeddings in each modality in the CIFAR-100 dataset (Krizhevsky, 2009; Krizhevsky
et al.)7 and the ImageNet dataset (Deng et al., 2009) using NNGS(ei, et, k). For such, we calculate
the mean image embedding for each class (ei). Then, we use the corresponding text embeddings et
to estimate NNGS(ei, et, k).

We also experiment with two methods to make new templates. The first is to change the original
templates to their negations, that is, we use variations of “not a [description] [photo/image] of X”.
The second method is to make templates from phrases unrelated to images or photos using variations
of famous film and music phrases. The complete list of prompts can be found in Appendix F.

In this situation, there are 100 points, but there are no blobs. In this experiment, we are interested in
low neighborhood sizes, as they already relate to inter-class positions. For this reason, we use k = 3.

Next, we evaluate the zero-shot classification accuracies using each single template, and compare
them with its NNGS(X, Y, k) for k = 3, as shown in Figures 6 and 7. We present the measures
ρ1, which is the Pearson’s correlation between NNGS and the zero-shot accuracy for each prompt
template considering only the original templates, and ρ2, which considers the added templates as
well. We also present results of similar experiments using CKA and GULP.

Interestingly, results indicate that using negative templates instead of positive templates leads to
accuracy differences of less than 0.02. However, the film/music templates had a lower accuracy.

We find a high correlation between NNGS and the zero-shot classification accuracies, as shown in
Figures 6 and 7. Importantly, the values of ρ2 can be seen as skewed because the added prompts
seem to form a series of poorly-performing elements. When only the original prompts are considered,
the obtained values are lower, but still significant. However, we highglight that the same experiments,
when performed with CKA or GULP, leads to a lower correlation, which indicates that NNGS was
able to identify similarities that are more related to the zero-shot classification performance.

The plots in Figures 6a and 7a form a trend, suggesting that the similarity NNGS(X, Y, k) is related
to the zero-shot classification accuracy. However, we note that this trend cannot be observed if
particular sets of templates are chosen, especially in some subsets of templates with high accuracy.
This behavior could be due to reaching an accuracy higher limit, similarly to the observations in
Figure 5.

We further experiment using the ImageNet embeddings generated by a BLIP-2 model (Li et al., 2023).
In these experiments, we used the output of the [CLS] token as the text embedding, and the mean
of all image token embeddings as the representative of each particular item. The results, shown in
Figure 8, indicate that NNGS has a higher correlation with the task-specific performance.

Importantly, although NNGS yields a higher ρ1 in BLIP-2 embeddings than in CLIP embeddings, the
zero-shot classificatio accuracy is not necessarily higher. This means that NNGS correlates with the
task-specific accuracy within the same embeddings, but not necessarily across different embedding

5The template list

is available at https://github.com/openai/CLIP/blob/main/data/

prompts.md.

6https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/clip
7The CIFAR-100 website does not cite a specific licence for usage. However, this data is distributed under
various packages and has been broadly used in research for benchmark purposes. We specifically use the
distribution available through Pytorch.

9

Under review as a conference paper at ICLR 2025

486
487

488
489
490
491
492
493

494
495
496
497
498
499

500
501
502
503
504

505
506
507
508
509
510

511
512
513
514
515
516

517
518
519
520
521
522

523
524
525
526
527

528
529
530
531
532
533

534
535
536
537
538
539

domains. Also, we note that CKA and GULP have ρ2 < ρ1, which seems to be caused by the erratic
behavior of movies/music-inspired prompts (green dots).

In all cases discussed in this section, the similarity NNGS(X, Y, k) has shown to correlate with the
task-specific accuracy value. In addition, NNGS(X, Y, k) can be used to evaluate the underlying
assumptions for each case (eb − ea = ed − ec for the analogy task, and et ≈ ei for the zero-
shot classification task). Our results indicate that the more the underlying assumptions are met (as
measured by NNGS(X, Y, k)), the greater is the system’s tendency to reach higher performances.

5 CONCLUSION

Our study introduces a novel approach, namely Nearest Neighborhood Graph Similarity (NNGS) to
evaluating the similarity of paired embedding spaces using GloVe, CLIP, and BLIP-2 models as case
studies. The similarity is measured using the average Jaccard similarity between the corresponding
nodes of k-neighborhood-induced graphs. The resulting measure informs how well these models
preserve the underlying structural properties of the data. Our findings demonstrate a strong corre-
lation between similarity and task-specific accuracy, indicating that maintaining structure within
corresponding embedding spaces is a possible route for achieving high performance in tasks such as
analogy calculation and cross-modal zero-shot classification.

We have compared our method with two previous approaches (CKA and GULP), showing that
NNGS is invariant to dimensionality changes, robust to changes in the hyperparameter, and can be
made invariant to point cloud sizes by using a relative neighborhood size instead of an absolute one.
Also, we have shown that changing the value of k can make NNGS focus on local or on global
transformations over data, and finding a value for k can be done by investigating the clusters found
within the dataset. We have shown that a well-tuned NNGS presents a greater correlation with
task-specific performances than CKA and GULP.

Importantly, the results shown here do not mean that NNGS is a superior similarity measure in all
possible cases. Rather, we note that each representation similarity measure draws from different
assumptions, hence is more adequate for a different situation. Consequently, different similarity
metrics can be used harmoniously to provide different viewpoints about embedding spaces.

Looking forward, our findings raise important questions about other practical applications of similarity
metrics. We surmise that they could serve as an effective stopping criteria in early-stopping strategies,
aiding in the prevention of overfitting in cross-modal learning. Also, they could offer insights into
identifying groups of words where analogy tasks excel, guiding future model development. Last,
similarity could inform broader discussions on prompt engineering, helping to provide explanations
regarding the underlying agents behind the performance of specific prompt templates.

REFERENCES

Claire Donnat and Susan P. Holmes. Tracking network dynamics : A survey of distances and similarity
metrics. 2018. URL https://api.semanticscholar.org/CorpusID:3782171.

Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word
representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings
of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1532–1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/
v1/D14-1162. URL https://aclanthology.org/D14-1162.

Kawin Ethayarajh, David Kristjanson Duvenaud, and Graeme Hirst. Towards understanding linear

word analogies. In Annual Meeting of the Association for Computational Linguistics, 2018.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Re-
search, pages 8748–8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/
radford21a.html.

10

Under review as a conference paper at ICLR 2025

540
541

542
543
544
545
546
547

548
549
550
551
552
553

554
555
556
557
558

559
560
561
562
563
564

565
566
567
568
569
570

571
572
573
574
575
576

577
578
579
580
581

582
583
584
585
586
587

588
589
590
591
592
593

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image
pre-training with frozen image encoders and large language models. In Proceedings of the 40th
International Conference on Machine Learning, ICML’23. JMLR.org, 2023.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. Problems with evaluation of
word embeddings using word similarity tasks. In Proceedings of the 1st Workshop on Evaluating
Vector-Space Representations for NLP. Association for Computational Linguistics, 2016. doi:
10.18653/v1/w16-2506. URL http://dx.doi.org/10.18653/v1/W16-2506.

Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel
Albanie, and Matthias Bethge. No "zero-shot" without exponential data: Pretraining concept
frequency determines multimodal model performance. 2024. doi: 10.48550/ARXIV.2404.04125.
URL https://arxiv.org/abs/2404.04125.

Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and
why vision-language models behave like bags-of-words, and what to do about it? In International
Conference on Learning Representations, 2023. URL https://openreview.net/forum?
id=KRLUvxh8uaX.

Yibo Jiang, Bryon Aragam, and Victor Veitch. Uncovering meanings of embeddings via partial
orthogonality. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
Advances in Neural Information Processing Systems, volume 36, pages 31988–32005. Curran Asso-
ciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/
2023/file/65a925049647eab0aa06a9faf1cd470b-Paper-Conference.pdf.

Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, and
Stefano Soatto. Linear spaces of meanings: Compositional structures in vision-language models.
In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, October 2023.
doi: 10.1109/iccv51070.2023.01412. URL http://dx.doi.org/10.1109/ICCV51070.
2023.01412.

Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neu-
ral network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi-
tors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15
June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Re-
search, pages 3519–3529. PMLR, 2019. URL http://proceedings.mlr.press/v97/
kornblith19a.html.

Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.

Svcca: Singu-
lar vector canonical correlation analysis for deep learning dynamics and interpretability.
In
I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/
2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.

Enric Boix-Adsera, Hannah Lawrence, George Stepaniants, and Philippe Rigollet. Gulp:
In S. Koyejo, S. Mohamed,
In-
Inc.,
URL https://proceedings.neurips.cc/paper_files/paper/2022/

a prediction-based metric between representations.
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
formation Processing Systems, volume 35, pages 7115–7127. Curran Associates,
2022.
file/2f0435cffef91068ced08d7c7d8e643e-Paper-Conference.pdf.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs.
toronto.edu/~kriz/learning-features-2009-TR.pdf.

Adir Rahamim and Yonatan Belinkov. Contrasim – analyzing neural representations based on

contrastive learning, 2023.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced

research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

11

Under review as a conference paper at ICLR 2025

Harold Hotelling. Relations Between Two Sets of Variates, page 162–190. Springer New York, 1992.
ISBN 9781461243809. doi: 10.1007/978-1-4612-4380-9_14. URL http://dx.doi.org/
10.1007/978-1-4612-4380-9_14.

D. Eppstein, M. S. Paterson, and F. F. Yao. On nearest-neighbor graphs. Discrete amp; Computational
ISSN 1432-0444. doi: 10.1007/pl00009293. URL

Geometry, 17(3):263–282, April 1997.
http://dx.doi.org/10.1007/PL00009293.

Mark Newman. Networks. Oxford University Press, October 2018.

ISBN 9780198805090.
doi: 10.1093/oso/9780198805090.001.0001. URL http://dx.doi.org/10.1093/oso/
9780198805090.001.0001.

Vlad Sobal, Mark Ibrahim, Randall Balestriero, Vivien Cabannes, Diane Bouchacourt, Pietro Astolfi,
Kyunghyun Cho, and Yann LeCun. X-sample contrastive loss: Improving contrastive learning
with sample similarity graphs, 2024.

Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. In International Conference on Learning Representations, 2013.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09, 2009.

12

594
595

596
597
598
599
600
601

602
603
604
605
606
607

608
609
610
611
612

613
614
615
616
617
618

619
620
621
622
623
624

625
626
627
628
629
630

631
632
633
634
635

636
637
638
639
640
641

642
643
644
645
646
647

Under review as a conference paper at ICLR 2025

A FIGURES WITH RESULTS

(a) NNGS. ρ = 0.86.

(b) CKA. ρ = 0.87.

(c) GULP. ρ = 0.14.

Figure 5: Pearson’s correlation between NNGS, CKA, and GULP and the accuracy in the analogy
task in GloVe embeddings. Each point represents an analogy task. The similarity measured by NNGS
and CKA have a high correlation to the analogy accuracy, whereas this trend is unobserved in GULP.

(a) NNGS. ρ1 = 0.78, ρ2 =
0.90.

(b) CKA. ρ1 = 0.71, ρ2 = 0.87.

(c) GULP. ρ1 = 0.68, ρ2 = 0.86.

Figure 6: Pearson’s correlation between NNGS, CKA, and GULP and zero-shot accuracy in the
CIFAR-100 dataset using CLIP embeddings. Each point represents one template (blue: original
templates, red: negative templages, green: templates inspired in movies and music). We report ρ1
related to using the original templates, and ρ2, related to using the original and the added templates.

(a) NNGS. ρ1 = 0.54, ρ2 =
0.91.

(b) CKA. ρ1 = 0.39, ρ2 = 0.74.

(c) GULP. ρ1 = 0.46, ρ2 = 0.80.

Figure 7: Pearson’s correlation between NNGS, CKA, and GULP and zero-shot accuracy in the
ImageNet dataset using CLIP embeddings. Each point represents one template (blue: original
templates, red: negative templages, green: templates inspired in movies and music). We report ρ1
related to using the original templates, and ρ2, related to using the original and the added templates.

13

648
649

650
651
652
653
654
655

656
657
658
659
660
661

662
663
664
665
666

667
668
669
670
671
672

673
674
675
676
677
678

679
680
681
682
683
684

685
686
687
688
689

690
691
692
693
694
695

696
697
698
699
700
701

0.30.40.50.6NNGS(X,Y,0.2n)0.00.20.40.60.81.0Analogy accuracy0.750.800.850.900.95CKA(A,B)0.00.20.40.60.81.0Analogy accuracy01234-GULP(A,B,=1)0.00.20.40.60.81.0Analogy accuracy0.200.250.300.350.40NNGS(ei,et,k=3)0.500.520.540.560.580.600.620.64Zero-shot Accuracy0.600.620.640.660.680.700.720.740.76CKA(ei,et)0.500.520.540.560.580.600.620.64Zero-shot Accuracy0.0340.0320.0300.0280.0260.0240.022-GULP(ei,et,=1)0.500.520.540.560.580.600.620.64Zero-shot Accuracy0.160.180.200.220.24NNGS(ei,et,k=3)0.4000.4250.4500.4750.5000.5250.5500.5750.600Zero-shot Accuracy0.5000.5250.5500.5750.6000.6250.6500.675CKA(ei,et)0.4000.4250.4500.4750.5000.5250.5500.5750.600Zero-shot Accuracy0.0300.0280.0260.0240.022-GULP(ei,et,=1)0.4000.4250.4500.4750.5000.5250.5500.5750.600Zero-shot AccuracyUnder review as a conference paper at ICLR 2025

702
703

704
705
706
707
708
709

710
711
712
713
714
715

716
717
718
719
720

721
722
723
724
725
726

727
728
729
730
731
732

733
734
735
736
737
738

739
740
741
742
743

744
745
746
747
748
749

750
751
752
753
754
755

(a) NNGS. ρ1 = 0.79, ρ2 =
0.91.

(b) CKA. ρ1 = 0.62, ρ2 = 0.46.

(c) GULP. ρ1 = 0.44, ρ2 = 0.41.

Figure 8: Pearson’s correlation between NNGS, CKA, and GULP and zero-shot accuracy in the
ImageNet dataset using BLIP-2 embeddings. Each point represents one template (blue: original
templates, red: negative templages, green: templates inspired in movies and music). We report ρ1
related to using the original templates, and ρ2, related to using the original and the added templates.

B PROOF OF EQUATION 7

Suppose we have a bag containing M numbered balls, which are numbered from 1 to M . We build
set A by randomly drawing k balls from the bag without replacement. Then, we put all balls back in
the ball and build set B by drawing the same number k of balls without replacement. The question is:
what is the cardinality of the intersection between A and B?

B.1 SOLUTION: HYPERGEOMETRIC DISTRIBUTION

After building set A, we can mark all balls with a cross before replacing them. Then, when building
set B, we have the following situation:

• We have M balls,

• There are k balls of interest,

• There are k draws without replacement

• x = |A ∩ B| corresponds to the number of successes within these draws.

This directly leads to a hypergeometric distribution:

p(x; M, k) = Hyp(x, M, k, k) =

(cid:0)k
x

(cid:1)

(cid:1)(cid:0)M −k
k−x
(cid:1)

(cid:0)M
k

And, following the properties of a hypergeometric distribution:

E[x] =

# objects of interest × # draws
# total objects

=

k2
M

B.2 THE CASE OF JACCARD SIMILARITY

The Jaccard Similarity between sets A and B, with k draws, is:

J(A, B, k) =

|A ∩ B|
|A ∪ B|

=

|A ∩ B|
|A| + |B| − |A ∩ B|

=

|A ∩ B|
2k − |A ∩ B|

.

Let x = |A ∩ B| and consider the function:

14

(9)

(10)

(11)

0.150.160.170.180.190.200.21NNGS(ei,et,k=3)0.460.480.500.520.54Zero-shot Accuracy0.500.550.600.65CKA(ei,et)0.460.480.500.520.54Zero-shot Accuracy0.0260.0240.0220.0200.0180.016-GULP(ei,et,=1)0.460.480.500.520.54Zero-shot AccuracyUnder review as a conference paper at ICLR 2025

f (x) = J(A, B, k) =

x
2k − x

This function is convex, hence, by Jensen’s inequality,

E[f (x)] ≥

E[x]
2k − E[x]

Substituting x using Equation 10, we have:

E[J(A, B, k)] ≥

k2/M
2k − (k2/M )

We can simplify:

k2/M
2k − (k2/M )

=

k2
M (2k − (k2/M ))

=

k2
2M k − k2 .

Since we know that k is strictly positive (k > 0), then:

E[J(A, B, k)] ≥

k
2M − k

B.3 CHOOSING k AS A FRACTION OF M

If k = cM, c ∈ {i/M |1 ≤ i ≤ M, i ∈ N}, then we have:

E[J(A, B, k)] ≥

cM
2M − cM

=

cM
(2 − c)M

=

c
2 − c

(12)

(13)

(14)

(15)

(16)

(17)

C PROOF THAT NEIGHBORHOOD CHOICE IS NOT INDEPENDENT

Let x1, x2, x3 ∈ Rd be 3 points in a point cloud. Using a distance measure γ(xi, xj), we induce a
k-neighbor graph with k = 1. Suppose we find the edges: (x1, x2) and (x2, x3). What is the edge
starting at x3?

Finding the aforementioned edges implies that γ(x1, x2) < γ(x1, x3) and γ(x2, x3) < γ(x1, x2).
Because distances are symmetric, γ(x1, x2) < γ(x3, x1) and γ(x3, x2) < γ(x1, x2).

Thus, γ(x3, x2) < γ(x1, x2) < γ(x3, x1). Henceforth the third edge of the graph must be (x3, x2).

This counter-example shows that neighborhoods are not entirely independent. Rather, there are
less possible choices than the total number of vertices in the graph as distance properties must be
respected.

D WHITE NOISE SWEEP

We further analyze the effects of changing the noise level by using a sweep, as follows. We fixed a
neighborhood value k = 20 and evaluated NNGS(X, Y, k) for various SNR values. The SNR sweep
shown in Figure 9 indicates that NNGS(X, Y, k) saturates in SNRs more extreme than -10 dB and 30
dB. This behavior is also found in Figure 1, which shows that the curves outside these bounds are
very close to the extremes NNGS(X, Y, k) = 1 and NNGS(X, Y, k) = H(k). This is an important
observation, as it determines the limits within NNGS(X, Y, k) can be effective.

15

756
757

758
759
760
761
762
763

764
765
766
767
768
769

770
771
772
773
774

775
776
777
778
779
780

781
782
783
784
785
786

787
788
789
790
791
792

793
794
795
796
797

798
799
800
801
802
803

804
805
806
807
808
809

Under review as a conference paper at ICLR 2025

Figure 9: Similarity NNGS(X, Y, k) as a function on the SNR used to distort the point cloud. Higher
noise levels (lower SNR) lead to lower similarity, while lower noise levels (higher SNR) lead to
higher similarity. Importantly, the measure saturates with SNR lower than -10 dB or higher than 30
dB.

E ALGORITHM AND PARAMETERS USED TO CREATE THE BLOBS IN

SECTION 3.3

The variables used in the CreateAlignedDataset procedure are:

• n_dim: integer. Number of dimensions for the aligned datasets

• n_items: list of integers. Number of items per data blob.
• µ1 and µ2: list of values. Blob centers for each data blob.
• σ1 and σ2: list of values. Blob spreads for each data blob.
• noise: float. Standard deviation of noise added to each data blob.

X1 ← empty array
X2 ← empty array
for i = 0 to len(n_items) − 1 do

Algorithm 1 Create Aligned Dataset
1: procedure CREATEALIGNEDDATASET(n_dim, n_items, µ1, σ1, µ2, σ2, noise)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end procedure

x ← random normal distribution(0, 1, (n_items[i], n_dim))
x1 ← x × σ1[i] + µ1[i]
x2 ← x × σ2[i] + µ2[i]
ϕ ← random normal distribution(0, 1, (n_items[i], n_dim)) × noise[i]
x2 ← x2 + ϕ
Append x1 to X1 as a new line
Append x2 to X2 as a new line

end for
return X1, X2

810
811

812
813
814
815
816
817

818
819
820
821
822
823

824
825
826
827
828

829
830
831
832
833
834

835
836
837
838
839
840

841
842
843
844
845
846

847
848
849
850
851

852
853
854
855
856
857

858
859
860
861
862
863

16

20100102030405060SNR (dB)0.00.20.40.60.81.0NNGS(X,Y,20)Under review as a conference paper at ICLR 2025

Blobs w/ Dif.
Scales

Unbal. Blobs w/
Dif. Scales

Noise Within Blobs

Shuffled Blobs

n_dim
n_items
mu1
sigma1
mu2
sigma2
noise

20
200, 200
-1, 3
3, 0.1
-1, 3
0.1, 3
0, 0

20
10, 1000
-1, 3
3, 0.1
-1, 3
0.1, 3
0, 0

20
100, 100, 100, 100
0, 1, 2, 3
0.1, 0.1, 0.1, 0.1
0, 1, 2, 3
0.1, 0.1, 0.1, 0.1
0.5, 0.5, 0.5, 0.5

20
100, 100, 100, 100
0, 1, 2, 3
0.1, 0.1, 0.1, 0.1
2, 1, 3, 0
0.1, 0.1, 0.1, 0.1
0, 0, 0, 0

Table 2: Parameters for experiments perfomred in Section 3.3.

F LIST OF PROMPTS USED IN SECTION 4.2

• a photo of a X.
• a blurry photo of a X.
• a black and white photo of a X.
• a low contrast photo of a X.
• a high contrast photo of a X.
• a bad photo of a X.
• a good photo of a X.
• a photo of a small X.
• a photo of a big X.
• a photo of the X.
• a blurry photo of the X.
• a black and white photo of the X.
• a low contrast photo of the X.
• a high contrast photo of the X.
• a bad photo of the X.
• a good photo of the X.
• a photo of the small X.
• a photo of the big X.
• not a photo of a X.
• not a blurry photo of a X.
• not a black and white photo of a X.
• not a low contrast photo of a X.
• not a high contrast photo of a X.
• not a bad photo of a X.
• not a good photo of a X.
• not a photo of a small X.
• not a photo of a big X.
• not a photo of the X.

• not a blurry photo of the X.

• not a black and white photo of the X.

• not a low contrast photo of the X.

• not a high contrast photo of the X.

• not a bad photo of the X.

• not a good photo of the X.

• not a photo of the small X.

• not a photo of the big X.

• Luke. I am your X.

• The sound of a X.

• Feel the power of a X.

• Born to be X.

• I am feeling supersonic, give me X and

tonic.

• Stop trying to make X happen!

• Perhaps X could help us save Robin

from The Joker.

• The wheels on the X go round and

round.

• We all live in a yellow X

• I can find the X here.

• X, we have a problem.

• You cannot handle the X.

• I am the X of the world.

• There is no place like X.

• You are a X, Harry!

• AND MY X!

• We must take the X to Mordor!

17

864
865

866
867
868
869
870
871

872
873
874
875
876
877

878
879
880
881
882

883
884
885
886
887
888

889
890
891
892
893
894

895
896
897
898
899
900

901
902
903
904
905

906
907
908
909
910
911

912
913
914
915
916
917

